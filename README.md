# MHParsNet

## Results
#### Comaprision of ```MHParsNet``` with state-of-the art algorithms using AP<sup>r</sup>, AP<sup>p</sup> and PCP
Except for [M-CE2P](https://github.com/RanTaimu/M-CE2P), the state-of-the-art algorithms enumerated in Table 1 could not be replicated. Some reasons include deprecated codes (libraries), inadequate documentation, and incompatible dependencies. For such algorithms (marked byâˆ—), the backbone network's size (parameter count) is provided to give a general sense of the model's scale and complexity. Therefore, the entire model is expected to be significantly larger.

![](results_table.png)

&nbsp;


#### Evaluation: the quality of the parsing (segmentation) masks predicted by ```MHParsNet```
![](segmentation_result.png)
**Figure 1.** Parsing (segmentation) result of MHParsNet on sample data from the [MHPV2](https://lv-mhp.github.io/dataset) validation dataset.
&nbsp;

Fig. 1 above shows the mask generation quality of MHParsNet. From Fig. 1, MHParsNet correctly detects and segments all human instances from the sample RGB image. Additionally, MHParsNet correctly detects and segments all but one body part. MHParsNet misses the ```Sunglasses``` object associated with the male subject, which reflects in the human and part instance mask prediction. However, this does not suggest that MHParsNet can not detect small objects; the abnormality is simply due to a false negative prediction. This assertion is validated by the fact that MHParsNet detects the relatively smaller ```Watch``` object on the wrist of the female subject. More test samples can be generated by executing ```test.py```.

&nbsp;
#### Evaluation: comparision of ```MHParsNet's``` mask quality with other state-of-the-art algorithms
As aforementioned, except for [M-CE2P](https://github.com/RanTaimu/M-CE2P), the state-of-the-art algorithms enumerated in Table 1 could not be replicated. Therefore, the evaluation of the segmentation mask quality is performed between ```MHParsNet``` and ```M-CE2P``` only. The evaluation is performed on the CIHP and MHPv2 datasets, but the result of ```3.jpg``` from the [MHPV2](https://lv-mhp.github.io/dataset) validation dataset only is showed here.

![](comparision.png)
**Figure 2.** Comparision of the segmentation (parsing) mask quality between ```MHParsNet```(ours) and ```M-CE2P```.

As shown in Fig. 2, `3.jpg` comprises two human instances; a male and a female subject. According to the ground truth annotation, the male subject possesses the following attributes: ```hair```, ```face```, ```Torso-skin```, ```Jacket/windbreaker/hoodie```, ```Singlet```,  ```right-hand```,  ```pants```, ```left-shoe``` and ```right-shoe```. The female subject on the other posses: ```hair```, ```face```, ```Singlet```, ```right-arm```, ```left-arm```, ```right-hand```, ```right-hand```, ```Backpack```, ```pants```, ```Belt```, ```left-shoe``` and ```right-shoe```. 

For the male subject, ```M-CE2P``` makes a **false negative** prediction on the ```Torso-skin``` class whiles on the female subject it makes the following **false negative** mask predictions: ```right-arm```, ```right-hand```, ```Backpack```, ```Belt```, ```left-shoe``` and ```right-shoe```. Additionally, it falsely predicts some object classes in the upper body part of the female subject. On the other hand, our ```MHParsNet```makes no false positive predictions in the upper body part of the female subject. Additionally, all but the ```Belt```, ```Torso-skin```, ```Right-arm``` and the ```Backpack``` classes are correctly predicted. For the male subject, ```MHParsNet```only misses the ``Torso-skin``` class. From a visual perspective, it can be observed that despite having fewer trainable parameters, ```MHParsNet``` predicts higher quality segmentation masks than ```M-CE2P```.



&nbsp;
#### Evaluation: deployment on [Jetson Nano](https://developer.nvidia.com/embedded/jetson-nano-developer-kit) embedded board
To evaluate the adaptability of MHParsNet on embedded devices, we conducted inference on a smart camera prototype using the [Jetson Nano](https://developer.nvidia.com/embedded/jetson-nano-developer-kit) embedded board and the [Logitecg C920 HD Pro Webcam](https://www.logitech.com/de-at/products/webcams/c920-pro-hd-webcam.960-001055.html). We used the [Ubuntu 20.04 OS image](https://github.com/Qengineering/Jetson-Nano-Ubuntu-20-image) from [Qengineering](https://github.com/Qengineering/Jetson-Nano-Ubuntu-20-image). ```MHParsNet``` achieved 6 FPS on average on the smart camera prototype. With up to four human instances, a maximum of 11 frames per second could be achieved. The details of the code implementation are in ```camera_view.py```.

![](webcam.gif)

