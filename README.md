# MHParsNet

## Results
#### Comaprision of ```MHParsNet``` with state-of-the art algorithms using AP<sup>r</sup>, AP<sup>p</sup> and PCP
Except for [M-CE2P](https://github.com/RanTaimu/M-CE2P), the state-of-the-art algorithms enumerated in Table 1 could not be replicated. Some reasons include deprecated codes (libraries), inadequate documentation, and incompatible dependencies. For such algorithms (marked byâˆ—), the backbone network's size (parameter count) is provided to give a general sense of the model's scale and complexity. Therefore, the entire model is expected to be significantly larger.

![](results_table.png)

&nbsp;


#### Evaluation: the quality of the parsing (segmentation) masks predicted by ```MHParsNet```
![](segmentation_result.png)
**Figure 1.** Parsing (segmentation) result of MHParsNet on sample data from the [MHPV2](https://lv-mhp.github.io/dataset) validation dataset.
&nbsp;

Fig. 1 above shows the mask generation quality of MHParsNet. From Fig. 1, MHParsNet correctly detects and segments all human instances from the sample RGB image. Additionally, MHParsNet correctly detects and segments all but one body part. MHParsNet misses the ```Sunglasses``` object associated with the male subject, which reflects in the human and part instance mask prediction. However, this does not suggest that MHParsNet can not detect small objects; the abnormality is simply due to a false negative prediction. This assertion is validated by the fact that MHParsNet detects the relatively smaller ```Watch``` object on the wrist of the female subject. More test samples can be generated by executing ```test.py```.

&nbsp;
#### Evaluation: comparision of ```MHParsNet's``` mask quality with other state-of-the-art algorithms
As aforementioned, except for [M-CE2P](https://github.com/RanTaimu/M-CE2P), the state-of-the-art algorithms enumerated in Table 1 could not be replicated. Therefore, the evaluation of the segmentation mask quality is performed between ```MHParsNet``` and ```M-CE2P``` only. The evaluation is performed on the CIHP and MHPv2 datasets, but the result of ```3.jpg``` from the [MHPV2](https://lv-mhp.github.io/dataset) validation dataset only is showed here.

![](comparision.png)
**Figure 2.** Comparision of the segmentation (parsing) mask quality between ```MHParsNet```(ours) and ```M-CE2P```.

As shown in Fig. 2, `3.jpg` comprises 2 human instances; a male and a femal subject. According to the groundtruth annotation, the male subject posseses the following attributes: ```hair```, ```face```, ```Torso-skin```, ```Jacket/windbreaker/hoodie```, ```Singlet```,  ```right-hand```,  ```pants```, ```left-shoe``` and ```right-shoe```. The female subject on the other posses: ```hair```, ```face```, ```Singlet```, ```right-arm```, ```left-arm```, ```right-hand```, ```right-hand```, ```Backpack```, ```pants```, ```Belt```, ```left-shoe``` and ```right-shoe```. 

For the male subject, M-CE2P makes a false negative prediction on the ```Torso-skin``` whiles on the female subject it makes the following false negative mask predictions: ```right-arm```, ```right-hand```, ```Backpack```, ```Belt```, ```left-shoe``` and ```right-shoe```. Additionally it falsely predicts some object classes in the upper body part of the female subject. Our ```MHParsNet``` on the ther hand makes no false positves class predictions in the upper body part of the female subject. Additionally all but the ```Belt```, ```Torso-skin```, ```right-arm``` and the ```Backpack``` classes are missed from the mask predictions. For the male subject, only the ```Torso-skin``` class is missed. From a visual perspective, it can be observed despite having fewer trainable parameters, ```MHParsNet``` predicts higher quality segmentation masks compared to ```M-CE2P```.



&nbsp;
#### Evaluation: deployment on [Jetson Nano](https://developer.nvidia.com/embedded/jetson-nano-developer-kit) embedded board
some data
